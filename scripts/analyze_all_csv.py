#!/usr/bin/env python3
"""
å…¨é¢åˆ†ææ‰€æœ‰CSVæ–‡ä»¶çš„è…³æœ¬
æ”¯æŒè·³éé‚è¼¯ã€æ‰¹é‡è™•ç†ã€è©³ç´°å ±å‘Šå’Œæ€§èƒ½ç›£æ§
"""
import sys
import os
from pathlib import Path
import time
import json
import pandas as pd
from datetime import datetime
import argparse

def setup_python_path():
    """è¨­ç½®Pythonè·¯å¾‘ä»¥ç¢ºä¿æ¨¡çµ„èƒ½è¢«æ­£ç¢ºå°å…¥"""
    project_root = Path(__file__).parent.parent.absolute()
    src_path = project_root / 'src'
    
    if str(src_path) not in sys.path:
        sys.path.insert(0, str(src_path))
    
    current_pythonpath = os.environ.get('PYTHONPATH', '')
    if str(src_path) not in current_pythonpath:
        os.environ['PYTHONPATH'] = str(src_path) + os.pathsep + current_pythonpath
    
    return src_path, project_root

def create_output_directories(base_output_dir):
    """å‰µå»ºè¼¸å‡ºç›®éŒ„çµæ§‹"""
    dirs = [
        base_output_dir,
        base_output_dir / 'images',
        base_output_dir / 'reports',
        base_output_dir / 'data'
    ]
    
    for dir_path in dirs:
        dir_path.mkdir(parents=True, exist_ok=True)
    
    return dirs

def scan_csv_files(data_dir):
    """æƒææ‰€æœ‰CSVæ–‡ä»¶"""
    csv_files = list(Path(data_dir).glob('*.csv'))
    return sorted(csv_files)

def generate_analysis_report(results, output_dir, processing_time):
    """ç”Ÿæˆè©³ç´°çš„åˆ†æå ±å‘Š"""
    
    # çµ±è¨ˆæ•¸æ“š
    total_files = len(results)
    successful = [r for r in results if r.get('success', False)]
    skipped = [r for r in results if r.get('skipped', False)]
    failed = [r for r in results if not r.get('success', False) and not r.get('skipped', False)]
    
    # å‰µå»ºçµ±è¨ˆæ‘˜è¦
    stats = {
        'total_files': total_files,
        'successful': len(successful),
        'skipped': len(skipped),
        'failed': len(failed),
        'success_rate': len(successful) / total_files * 100 if total_files > 0 else 0,
        'skip_rate': len(skipped) / total_files * 100 if total_files > 0 else 0,
        'processing_time': processing_time,
        'average_time_per_file': processing_time / total_files if total_files > 0 else 0,
        'timestamp': datetime.now().isoformat()
    }
    
    # ä¿å­˜çµ±è¨ˆæ‘˜è¦
    with open(output_dir / 'reports' / 'analysis_stats.json', 'w') as f:
        json.dump(stats, f, indent=2)
    
    # å‰µå»ºè©³ç´°çµæœCSV
    results_df = pd.DataFrame(results)
    results_df.to_csv(output_dir / 'reports' / 'detailed_results.csv', index=False)
    
    # å‰µå»ºæˆåŠŸæ–‡ä»¶çš„è©³ç´°åˆ†æ
    if successful:
        success_data = []
        for result in successful:
            success_data.append({
                'dataid': result.get('dataid'),
                'I_c': result.get('I_c'),
                'I_c_error': result.get('I_c_error'),
                'r_squared': result.get('r_squared'),
                'chi_squared': result.get('chi_squared'),
                'processing_time': result.get('processing_time', 0)
            })
        
        success_df = pd.DataFrame(success_data)
        success_df.to_csv(output_dir / 'reports' / 'successful_analysis.csv', index=False)
        
        # è¨ˆç®—çµ±è¨ˆæŒ‡æ¨™
        i_c_stats = {
            'mean': float(success_df['I_c'].mean()) if len(success_df) > 0 else 0,
            'std': float(success_df['I_c'].std()) if len(success_df) > 0 else 0,
            'min': float(success_df['I_c'].min()) if len(success_df) > 0 else 0,
            'max': float(success_df['I_c'].max()) if len(success_df) > 0 else 0,
            'median': float(success_df['I_c'].median()) if len(success_df) > 0 else 0
        }
        
        r_squared_stats = {
            'mean': float(success_df['r_squared'].mean()) if len(success_df) > 0 else 0,
            'std': float(success_df['r_squared'].std()) if len(success_df) > 0 else 0,
            'min': float(success_df['r_squared'].min()) if len(success_df) > 0 else 0,
            'max': float(success_df['r_squared'].max()) if len(success_df) > 0 else 0,
            'median': float(success_df['r_squared'].median()) if len(success_df) > 0 else 0
        }
        
        stats['i_c_statistics'] = i_c_stats
        stats['r_squared_statistics'] = r_squared_stats
    
    # å‰µå»ºè·³éæ–‡ä»¶åˆ†æ
    if skipped:
        skip_reasons = {}
        for result in skipped:
            error = result.get('error', 'Unknown')
            if error in skip_reasons:
                skip_reasons[error] += 1
            else:
                skip_reasons[error] = 1
        
        stats['skip_reasons'] = skip_reasons
    
    # å‰µå»ºå¤±æ•—æ–‡ä»¶åˆ†æ
    if failed:
        failure_reasons = {}
        for result in failed:
            error = result.get('error', 'Unknown')
            if error in failure_reasons:
                failure_reasons[error] += 1
            else:
                failure_reasons[error] = 1
        
        stats['failure_reasons'] = failure_reasons
    
    # é‡æ–°ä¿å­˜å®Œæ•´çµ±è¨ˆ
    with open(output_dir / 'reports' / 'analysis_stats.json', 'w') as f:
        json.dump(stats, f, indent=2)
    
    return stats

def print_progress_report(stats):
    """æ‰“å°é€²åº¦å ±å‘Š"""
    print("\n" + "=" * 80)
    print("ğŸ“Š å…¨é¢åˆ†æå®Œæˆå ±å‘Š")
    print("=" * 80)
    
    print(f"ğŸ“ ç¸½æ–‡ä»¶æ•¸: {stats['total_files']}")
    print(f"âœ… æˆåŠŸè™•ç†: {stats['successful']} ({stats['success_rate']:.1f}%)")
    print(f"â­ï¸  è³ªé‡è·³é: {stats['skipped']} ({stats['skip_rate']:.1f}%)")
    print(f"âŒ è™•ç†å¤±æ•—: {stats['failed']}")
    print(f"â±ï¸  ç¸½è™•ç†æ™‚é–“: {stats['processing_time']:.2f} ç§’")
    print(f"âš¡ å¹³å‡è™•ç†æ™‚é–“: {stats['average_time_per_file']:.3f} ç§’/æ–‡ä»¶")
    
    if 'i_c_statistics' in stats:
        print(f"\nğŸ”¬ I_c çµ±è¨ˆ (æˆåŠŸè™•ç†çš„æ–‡ä»¶):")
        i_c_stats = stats['i_c_statistics']
        print(f"   å¹³å‡å€¼: {i_c_stats['mean']:.3e}")
        print(f"   æ¨™æº–å·®: {i_c_stats['std']:.3e}")
        print(f"   ç¯„åœ: {i_c_stats['min']:.3e} ~ {i_c_stats['max']:.3e}")
        print(f"   ä¸­ä½æ•¸: {i_c_stats['median']:.3e}")
    
    if 'r_squared_statistics' in stats:
        print(f"\nğŸ“ˆ RÂ² çµ±è¨ˆ (æ“¬åˆè³ªé‡):")
        r2_stats = stats['r_squared_statistics']
        print(f"   å¹³å‡å€¼: {r2_stats['mean']:.4f}")
        print(f"   æ¨™æº–å·®: {r2_stats['std']:.4f}")
        print(f"   ç¯„åœ: {r2_stats['min']:.4f} ~ {r2_stats['max']:.4f}")
        print(f"   ä¸­ä½æ•¸: {r2_stats['median']:.4f}")
    
    if 'skip_reasons' in stats:
        print(f"\nâ­ï¸  è·³éåŸå› åˆ†æ:")
        for reason, count in stats['skip_reasons'].items():
            print(f"   {count:3d} æ–‡ä»¶: {reason}")
    
    if 'failure_reasons' in stats:
        print(f"\nâŒ å¤±æ•—åŸå› åˆ†æ:")
        for reason, count in stats['failure_reasons'].items():
            print(f"   {count:3d} æ–‡ä»¶: {reason}")

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='å…¨é¢åˆ†ææ‰€æœ‰CSVæ–‡ä»¶')
    parser.add_argument('--data-dir', type=str, default='data/Ic', 
                       help='CSVæ–‡ä»¶ç›®éŒ„ (é»˜èª: data/Ic)')
    parser.add_argument('--output-dir', type=str, default='output/full_analysis', 
                       help='è¼¸å‡ºç›®éŒ„ (é»˜èª: output/full_analysis)')
    parser.add_argument('--max-workers', type=int, default=None,
                       help='æœ€å¤§å·¥ä½œç·šç¨‹æ•¸ (é»˜èª: è‡ªå‹•æª¢æ¸¬)')
    parser.add_argument('--sample-size', type=int, default=None,
                       help='æ¨£æœ¬å¤§å°é™åˆ¶ (ç”¨æ–¼æ¸¬è©¦, é»˜èª: è™•ç†æ‰€æœ‰æ–‡ä»¶)')
    parser.add_argument('--dry-run', action='store_true',
                       help='ä¹¾é‹è¡Œæ¨¡å¼ï¼Œåªé¡¯ç¤ºçµ±è¨ˆä¸å¯¦éš›è™•ç†')
    
    args = parser.parse_args()
    
    print("ğŸ”§ å…¨é¢CSVåˆ†æè…³æœ¬å•Ÿå‹•")
    print("=" * 60)
    
    # è¨­ç½®Pythonè·¯å¾‘
    src_path, project_root = setup_python_path()
    print(f"âœ“ Pythonè·¯å¾‘è¨­ç½®: {src_path}")
    
    # æ¸¬è©¦æ¨¡çµ„å°å…¥
    try:
        from cpr.main_processor_optimized import EnhancedJosephsonProcessor, MAX_WORKERS
        print("âœ“ æˆåŠŸå°å…¥ EnhancedJosephsonProcessor å’Œ MAX_WORKERS")
    except ImportError as e:
        print(f"âŒ å°å…¥å¤±æ•—: {e}")
        return 1
    
    # æƒæCSVæ–‡ä»¶
    data_dir = project_root / args.data_dir
    if not data_dir.exists():
        print(f"âŒ æ•¸æ“šç›®éŒ„ä¸å­˜åœ¨: {data_dir}")
        return 1
    
    csv_files = scan_csv_files(data_dir)
    print(f"ğŸ“ æ‰¾åˆ° {len(csv_files)} å€‹CSVæ–‡ä»¶")
    
    if not csv_files:
        print("âŒ æ²’æœ‰æ‰¾åˆ°CSVæ–‡ä»¶")
        return 1
    
    # æ¨£æœ¬é™åˆ¶ï¼ˆç”¨æ–¼æ¸¬è©¦ï¼‰
    if args.sample_size and args.sample_size < len(csv_files):
        csv_files = csv_files[:args.sample_size]
        print(f"ğŸ”¬ æ¸¬è©¦æ¨¡å¼ï¼šåªè™•ç†å‰ {len(csv_files)} å€‹æ–‡ä»¶")
    
    # ä¹¾é‹è¡Œæ¨¡å¼
    if args.dry_run:
        print("ğŸƒ ä¹¾é‹è¡Œæ¨¡å¼ï¼šåˆ†ææ–‡ä»¶ä½†ä¸å¯¦éš›è™•ç†")
        file_sizes = []
        for file_path in csv_files:
            try:
                size = file_path.stat().st_size
                file_sizes.append(size)
            except:
                file_sizes.append(0)
        
        total_size = sum(file_sizes)
        print(f"ğŸ“Š æ–‡ä»¶çµ±è¨ˆ:")
        print(f"   ç¸½æ•¸: {len(csv_files)}")
        print(f"   ç¸½å¤§å°: {total_size / 1024 / 1024:.2f} MB")
        print(f"   å¹³å‡å¤§å°: {total_size / len(csv_files) / 1024:.2f} KB")
        print(f"   æœ€å¤§æ–‡ä»¶: {max(file_sizes) / 1024:.2f} KB")
        print(f"   æœ€å°æ–‡ä»¶: {min(file_sizes) / 1024:.2f} KB")
        return 0
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    output_dir = project_root / args.output_dir
    create_output_directories(output_dir)
    print(f"ğŸ“‚ è¼¸å‡ºç›®éŒ„: {output_dir}")
    
    # å‰µå»ºè™•ç†å™¨
    processor = EnhancedJosephsonProcessor()
    
    if args.max_workers:
        print(f"ğŸ”§ æ³¨æ„: max_workers åƒæ•¸ ({args.max_workers}) å·²æä¾›ï¼Œä½†è™•ç†å™¨ä½¿ç”¨å›ºå®šçš„ MAX_WORKERS = {MAX_WORKERS}")
    
    print(f"âš™ï¸  è™•ç†å™¨é…ç½®:")
    print(f"   æœ€å¤§å·¥ä½œç·šç¨‹: {MAX_WORKERS}")
    print(f"   ä½¿ç”¨Numbaå„ªåŒ–: {hasattr(processor, 'config')}")
    
    # é–‹å§‹è™•ç†
    print("\n" + "=" * 60)
    print("ğŸš€ é–‹å§‹å…¨é¢åˆ†æ")
    print("=" * 60)
    
    start_time = time.time()
    
    try:
        # è½‰æ›ç‚ºå­—ç¬¦ä¸²è·¯å¾‘
        file_paths = [str(f) for f in csv_files]
        
        # è™•ç†æ–‡ä»¶
        results = processor.process_files(file_paths, str(output_dir / 'images'))
        
        processing_time = time.time() - start_time
        
        # ç”Ÿæˆå ±å‘Š
        print("\nğŸ”„ ç”Ÿæˆåˆ†æå ±å‘Š...")
        stats = generate_analysis_report(results, output_dir, processing_time)
        
        # æ‰“å°å ±å‘Š
        print_progress_report(stats)
        
        # ä¿å­˜çµæœæ‘˜è¦
        print(f"\nğŸ’¾ å ±å‘Šå·²ä¿å­˜åˆ°:")
        print(f"   çµ±è¨ˆæ‘˜è¦: {output_dir / 'reports' / 'analysis_stats.json'}")
        print(f"   è©³ç´°çµæœ: {output_dir / 'reports' / 'detailed_results.csv'}")
        if stats['successful'] > 0:
            print(f"   æˆåŠŸåˆ†æ: {output_dir / 'reports' / 'successful_analysis.csv'}")
        print(f"   åœ–ç‰‡è¼¸å‡º: {output_dir / 'images'}")
        
        print(f"\nğŸ‰ åˆ†æå®Œæˆï¼ç¸½å…±è™•ç† {len(results)} å€‹æ–‡ä»¶")
        
        return 0
        
    except Exception as e:
        print(f"\nâŒ è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit(main())
